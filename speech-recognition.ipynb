{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<span style=\"color:red\">**THIS NOTEBOOK WORKS WITH MacOS 13.5.2 (22G91) ON THE Intel CHIP AND Python 3.7.17. OTHER OPERATIONAL SYSTEMS AND Python VERSIONS ARE NOT GUARANTEED TO WORK.**</span>."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3a7b92db3a19c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with installing and importing required libraries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ba278fa1f6b3af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "!pip install deepspeech pocketsphinx vosk librosa jiwer noisereduce==2.0.1 torch scipy pandas speechbrain transformers",
   "metadata": {
    "collapsed": false
   },
   "id": "11b5f8a46f3969bc"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e9c26157108630f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T17:31:34.687481Z",
     "start_time": "2023-12-10T17:31:34.678599Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "from deepspeech import Model as DeepspeechModel\n",
    "from pocketsphinx import Decoder\n",
    "from vosk import Model as VoskModel, KaldiRecognizer\n",
    "from speechbrain.pretrained import EncoderASR\n",
    "import noisereduce as nr\n",
    "import re\n",
    "import unicodedata\n",
    "from jiwer import wer as calculate_wer\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define transcriptions for comparing with recognized text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a082f7df40a8f7ac"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "transcriptions = {\n",
    "    # EN\n",
    "    'checkin.wav': 'Where is the check-in desk?',\n",
    "    'parents.wav': 'I have lost my parents.',\n",
    "    'suitcase.wav': 'Please, I have lost my suitcase.',\n",
    "    'what_time.wav': 'What time is my plane?',\n",
    "    'where.wav': 'Where are the restaurants and shops?',\n",
    "    'your_sentence1.wav': 'Hello Everyone, this is the sentence number one',\n",
    "    'your_sentence2.wav': 'How is it going?',\n",
    "    # IT\n",
    "    'checkin_it.wav': 'Dove e\\' il bancone?',\n",
    "    'parents_it.wav': 'Ho perso i miei genitori.',\n",
    "    'suitcase_it.wav': 'Per favore, ho perso la mia valigia.',\n",
    "    'what_time_it.wav': 'A che ora e’ il mio aereo?',\n",
    "    'where_it.wav': 'Dove sono i ristoranti e i negozi?',\n",
    "    # ES\n",
    "    'checkin_es.wav': '¿Dónde están los mostradores?',\n",
    "    'parents_es.wav': 'He perdido a mis padres.',\n",
    "    'suitcase_es.wav': 'Por favor, he perdido mi maleta.',\n",
    "    'what_time_es.wav': '¿A qué hora es mi avión?',\n",
    "    'where_es.wav': '¿Dónde están los restaurantes y las tiendas?'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:49:20.122961Z",
     "start_time": "2023-12-10T17:49:20.113302Z"
    }
   },
   "id": "53b7ff1da7ada"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For correct computation of WER we need to transform both transcription and recognized text to some unified form. So we define function `normalize_text` for that purpose."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c5b8688e6cc6d79"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b589a84bf879e6ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T17:49:33.115262Z",
     "start_time": "2023-12-10T17:49:33.108896Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    normalized_text = text.lower()\n",
    "    normalized_text = unicodedata.normalize('NFD', normalized_text)\n",
    "    normalized_text = ''.join([c for c in normalized_text if not unicodedata.combining(c)])\n",
    "    normalized_text = re.sub('[^a-z ]', '', normalized_text).lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Init ASRs we are working with. At the result we have something similar to Parameters Grid for evaluation of different ASR models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73aca55abf3c0f7e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def create_deepspeech_model_en():\n",
    "    model = DeepspeechModel('models/deepspeech/deepspeech-0.9.3-models.pbmm')\n",
    "    model.enableExternalScorer('models/deepspeech/deepspeech-0.9.3-models.scorer')\n",
    "    return model\n",
    "\n",
    "def create_deepspeech_model_es():\n",
    "    model = DeepspeechModel('models/deepspeech/output_graph_es.pbmm')\n",
    "    model.enableExternalScorer('models/deepspeech/kenlm_es.scorer',)\n",
    "    return model\n",
    "\n",
    "def create_deepspeech_model_it():\n",
    "    model = DeepspeechModel('models/deepspeech/output_graph_it.pbmm')\n",
    "    model.enableExternalScorer('models/deepspeech/kenlm_it.scorer')\n",
    "    return model\n",
    "\n",
    "def recognize_speech_with_deepspeech(model, signal):\n",
    "    return model.stt(signal)\n",
    "\n",
    "def recognize_speech_with_deepspeech_it(model, signal):\n",
    "    result = max(model.sttWithMetadata(signal, 5).transcripts, key=lambda t: len(t.tokens)).tokens\n",
    "    return''.join([token.text for token in result])\n",
    "\n",
    "def create_pocketsphinx_model():\n",
    "    return Decoder()\n",
    "\n",
    "def recognize_speech_with_pocketsphinx(model, signal):\n",
    "    signal_bytes= signal.tobytes()\n",
    "    model.reinit()\n",
    "    model.start_utt()\n",
    "    for i, _ in enumerate(signal_bytes[::2048]):\n",
    "        model.process_raw(signal_bytes[i * 2048:(i + 1) * 2048])\n",
    "    model.end_utt()\n",
    "    return model.hyp().hypstr\n",
    "\n",
    "def create_vosk_model():\n",
    "    return KaldiRecognizer(VoskModel(lang=\"en-us\"), 16000)\n",
    "\n",
    "def recognize_speech_with_vosk(model, signal):\n",
    "    signal_bytes= signal.tobytes()\n",
    "    model.Reset()\n",
    "    for i, _ in enumerate(signal_bytes[::4000]):\n",
    "        model.AcceptWaveform(signal_bytes[i * 4000:(i + 1) * 4000])\n",
    "\n",
    "    return json.loads(model.FinalResult())[\"text\"]\n",
    "\n",
    "def create_speechbrain_model():\n",
    "    return EncoderASR.from_hparams(\n",
    "        source=\"speechbrain/asr-wav2vec2-commonvoice-14-en\",\n",
    "        savedir=\"pretrained_models/asr-wav2vec2-commonvoice-14-en\"\n",
    "    )\n",
    "\n",
    "def recognize_speech_with_speechbrain(model, signal):\n",
    "    waveform = model.audio_normalizer(torch.tensor(signal), 16000)\n",
    "    result = model.transcribe_batch(waveform.unsqueeze(0), torch.tensor([1.0]))[0]\n",
    "    return str(result[0])\n",
    "\n",
    "models = {\n",
    "    'deepspeech': {\n",
    "        'EN': {\n",
    "            'create_model': create_deepspeech_model_en,\n",
    "            'recognize_speech': recognize_speech_with_deepspeech,\n",
    "            'use_noise_processing': True\n",
    "        },\n",
    "        'ES': {\n",
    "            'create_model': create_deepspeech_model_es,\n",
    "            'recognize_speech': recognize_speech_with_deepspeech,\n",
    "            'use_noise_processing': True\n",
    "        },\n",
    "        'IT': {\n",
    "            'create_model': create_deepspeech_model_it,\n",
    "            'recognize_speech': recognize_speech_with_deepspeech_it,\n",
    "            'use_noise_processing': True\n",
    "        }\n",
    "    },\n",
    "    'pocketsphinx': {\n",
    "        'EN': {\n",
    "            'create_model': create_pocketsphinx_model,\n",
    "            'recognize_speech': recognize_speech_with_pocketsphinx,\n",
    "            'use_noise_processing': False\n",
    "        }\n",
    "    },\n",
    "    'vosk': {\n",
    "        'EN': {\n",
    "            'create_model': create_vosk_model,\n",
    "            'recognize_speech': recognize_speech_with_vosk,\n",
    "            'use_noise_processing': False\n",
    "        }\n",
    "    },\n",
    "    'speechbrain': {\n",
    "        'EN': {\n",
    "            'create_model': create_speechbrain_model,\n",
    "            'recognize_speech': recognize_speech_with_speechbrain,\n",
    "            'use_noise_processing': True\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:49:35.781082Z",
     "start_time": "2023-12-10T17:49:35.771331Z"
    }
   },
   "id": "fef705aa6d6874e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define function for noise processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4a6e11b87759ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def process_noise(signal, samplerate, language):\n",
    "    normalized_signal = librosa.util.normalize(signal)\n",
    "\n",
    "    if language == 'EN':\n",
    "        return np.concatenate((\n",
    "            np.zeros(np.round(samplerate * 0.3).astype(np.int32), dtype=np.float32),\n",
    "            nr.reduce_noise(\n",
    "                y=normalized_signal,\n",
    "                sr=samplerate,\n",
    "                hop_length=128,\n",
    "                n_fft=512,\n",
    "                win_length=512,\n",
    "                prop_decrease=0.9,\n",
    "                time_constant_s=0.1,\n",
    "                freq_mask_smooth_hz = 6000\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    if language == 'ES':\n",
    "        return np.concatenate((\n",
    "            np.zeros(np.round(samplerate * 0.3).astype(np.int32), dtype=np.float32),\n",
    "            nr.reduce_noise(\n",
    "                y=normalized_signal,\n",
    "                sr=samplerate,\n",
    "                hop_length=128,\n",
    "                n_fft=512,\n",
    "                win_length=512,\n",
    "                prop_decrease=0.9,\n",
    "                time_constant_s=1,\n",
    "                freq_mask_smooth_hz = 1500\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    if language == 'IT':\n",
    "        return np.concatenate((\n",
    "            np.zeros(np.round(samplerate * 0.3).astype(np.int32), dtype=np.float32),\n",
    "            nr.reduce_noise(\n",
    "                y=normalized_signal,\n",
    "                sr=samplerate,\n",
    "                hop_length=128,\n",
    "                n_fft=512,\n",
    "                win_length=512,\n",
    "                prop_decrease=0.1,\n",
    "                time_constant_s=1,\n",
    "                freq_mask_smooth_hz = 100\n",
    "            )\n",
    "        ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:49:45.197099Z",
     "start_time": "2023-12-10T17:49:45.189539Z"
    }
   },
   "id": "4a6fe2f7e6a63bf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run evaluation of different ASR models. At the result we should have a table with each ASR model and language required. That table also should contain WER rate and time spent to recognize speech."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e9df872ab68e9e9"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a51b4b0cce30e25f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T17:56:30.926615Z",
     "start_time": "2023-12-10T17:55:13.588869Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow: v2.3.0-6-g23ad988fcd\n",
      "DeepSpeech: v0.9.3-0-gf2e9c858\n",
      "TensorFlow: v2.3.0-6-g23ad988fcd\n",
      "DeepSpeech: v0.9.3-0-gf2e9c858\n",
      "TensorFlow: v2.3.0-6-g23ad988fcd\n",
      "DeepSpeech: v0.9.3-0-gf2e9c858\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /Users/bastrich/.cache/vosk/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /Users/bastrich/.cache/vosk/vosk-model-small-en-us-0.15/graph/HCLr.fst /Users/bastrich/.cache/vosk/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /Users/bastrich/.cache/vosk/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing Wav2Vec2Model: ['project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.weight', 'project_hid.bias', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    },
    {
     "data": {
      "text/plain": "           Model Language                File  Time Spent, ms   wer\n0     deepspeech       EN           where.wav            3130    0%\n1     deepspeech       EN         parents.wav            1951   20%\n2     deepspeech       EN  your_sentence2.wav             983   25%\n3     deepspeech       EN  your_sentence1.wav            2044   50%\n4     deepspeech       EN        suitcase.wav            1745    0%\n5     deepspeech       EN         checkin.wav            1822   20%\n6     deepspeech       EN       what_time.wav            1266   20%\n7     deepspeech       EN                   *            1849   19%\n8     deepspeech       ES      parents_es.wav            2467    0%\n9     deepspeech       ES    what_time_es.wav            1384   83%\n10    deepspeech       ES     suitcase_es.wav            1763    0%\n11    deepspeech       ES        where_es.wav            2129    0%\n12    deepspeech       ES      checkin_es.wav            1424    0%\n13    deepspeech       ES                   *            1833   17%\n14    deepspeech       IT      parents_it.wav            1930    0%\n15    deepspeech       IT    what_time_it.wav             910   14%\n16    deepspeech       IT        where_it.wav            1323   29%\n17    deepspeech       IT     suitcase_it.wav            1291    0%\n18    deepspeech       IT      checkin_it.wav            1003   50%\n19    deepspeech       IT                   *            1292   19%\n20  pocketsphinx       EN           where.wav            2424   33%\n21  pocketsphinx       EN         parents.wav            4302   80%\n22  pocketsphinx       EN  your_sentence2.wav             735   25%\n23  pocketsphinx       EN  your_sentence1.wav            1606   62%\n24  pocketsphinx       EN        suitcase.wav            3644  100%\n25  pocketsphinx       EN         checkin.wav            3616   60%\n26  pocketsphinx       EN       what_time.wav            3995  100%\n27  pocketsphinx       EN                   *            2903   66%\n28          vosk       EN           where.wav             967    0%\n29          vosk       EN         parents.wav             493   20%\n30          vosk       EN  your_sentence2.wav             170    0%\n31          vosk       EN  your_sentence1.wav             594    0%\n32          vosk       EN        suitcase.wav             627   17%\n33          vosk       EN         checkin.wav             418   40%\n34          vosk       EN       what_time.wav             260    0%\n35          vosk       EN                   *             504   11%\n36   speechbrain       EN           where.wav            2004    0%\n37   speechbrain       EN         parents.wav            1965    0%\n38   speechbrain       EN  your_sentence2.wav            1239    0%\n39   speechbrain       EN  your_sentence1.wav            2205    0%\n40   speechbrain       EN        suitcase.wav            2212    0%\n41   speechbrain       EN         checkin.wav            2085   20%\n42   speechbrain       EN       what_time.wav            1575   20%\n43   speechbrain       EN                   *            1898    6%",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Language</th>\n      <th>File</th>\n      <th>Time Spent, ms</th>\n      <th>wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>where.wav</td>\n      <td>3130</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>parents.wav</td>\n      <td>1951</td>\n      <td>20%</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>your_sentence2.wav</td>\n      <td>983</td>\n      <td>25%</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>your_sentence1.wav</td>\n      <td>2044</td>\n      <td>50%</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>suitcase.wav</td>\n      <td>1745</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>checkin.wav</td>\n      <td>1822</td>\n      <td>20%</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>what_time.wav</td>\n      <td>1266</td>\n      <td>20%</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>deepspeech</td>\n      <td>EN</td>\n      <td>*</td>\n      <td>1849</td>\n      <td>19%</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>deepspeech</td>\n      <td>ES</td>\n      <td>parents_es.wav</td>\n      <td>2467</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>deepspeech</td>\n      <td>ES</td>\n      <td>what_time_es.wav</td>\n      <td>1384</td>\n      <td>83%</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>deepspeech</td>\n      <td>ES</td>\n      <td>suitcase_es.wav</td>\n      <td>1763</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>deepspeech</td>\n      <td>ES</td>\n      <td>where_es.wav</td>\n      <td>2129</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>deepspeech</td>\n      <td>ES</td>\n      <td>checkin_es.wav</td>\n      <td>1424</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>deepspeech</td>\n      <td>ES</td>\n      <td>*</td>\n      <td>1833</td>\n      <td>17%</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>deepspeech</td>\n      <td>IT</td>\n      <td>parents_it.wav</td>\n      <td>1930</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>deepspeech</td>\n      <td>IT</td>\n      <td>what_time_it.wav</td>\n      <td>910</td>\n      <td>14%</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>deepspeech</td>\n      <td>IT</td>\n      <td>where_it.wav</td>\n      <td>1323</td>\n      <td>29%</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>deepspeech</td>\n      <td>IT</td>\n      <td>suitcase_it.wav</td>\n      <td>1291</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>deepspeech</td>\n      <td>IT</td>\n      <td>checkin_it.wav</td>\n      <td>1003</td>\n      <td>50%</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>deepspeech</td>\n      <td>IT</td>\n      <td>*</td>\n      <td>1292</td>\n      <td>19%</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>where.wav</td>\n      <td>2424</td>\n      <td>33%</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>parents.wav</td>\n      <td>4302</td>\n      <td>80%</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>your_sentence2.wav</td>\n      <td>735</td>\n      <td>25%</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>your_sentence1.wav</td>\n      <td>1606</td>\n      <td>62%</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>suitcase.wav</td>\n      <td>3644</td>\n      <td>100%</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>checkin.wav</td>\n      <td>3616</td>\n      <td>60%</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>what_time.wav</td>\n      <td>3995</td>\n      <td>100%</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>pocketsphinx</td>\n      <td>EN</td>\n      <td>*</td>\n      <td>2903</td>\n      <td>66%</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>where.wav</td>\n      <td>967</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>parents.wav</td>\n      <td>493</td>\n      <td>20%</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>your_sentence2.wav</td>\n      <td>170</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>your_sentence1.wav</td>\n      <td>594</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>suitcase.wav</td>\n      <td>627</td>\n      <td>17%</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>checkin.wav</td>\n      <td>418</td>\n      <td>40%</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>what_time.wav</td>\n      <td>260</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>vosk</td>\n      <td>EN</td>\n      <td>*</td>\n      <td>504</td>\n      <td>11%</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>where.wav</td>\n      <td>2004</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>parents.wav</td>\n      <td>1965</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>your_sentence2.wav</td>\n      <td>1239</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>your_sentence1.wav</td>\n      <td>2205</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>suitcase.wav</td>\n      <td>2212</td>\n      <td>0%</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>checkin.wav</td>\n      <td>2085</td>\n      <td>20%</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>what_time.wav</td>\n      <td>1575</td>\n      <td>20%</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>speechbrain</td>\n      <td>EN</td>\n      <td>*</td>\n      <td>1898</td>\n      <td>6%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_report = []\n",
    "\n",
    "for model_name, languages in models.items():\n",
    "    for language, model_tools in languages.items():\n",
    "        wers = []\n",
    "        time_spents = []\n",
    "  \n",
    "        model = model_tools['create_model']()\n",
    "    \n",
    "        for audio_file in [file for file in os.listdir(f'audio/{language}') if file.endswith('.wav')]:\n",
    "            signal, _ = librosa.load(f'audio/{language}/{audio_file}', sr=16000)\n",
    "            \n",
    "            if model_tools['use_noise_processing']:\n",
    "                signal = process_noise(signal, 16000, language)\n",
    "                \n",
    "            signal = (signal * 32767).astype(np.int16)\n",
    "\n",
    "            start = time.time_ns()\n",
    "            recognized_text = model_tools['recognize_speech'](model, signal)\n",
    "            end = time.time_ns()\n",
    "            \n",
    "            time_spent_ms = (end - start) / 1000000\n",
    "            time_spents.append(time_spent_ms)\n",
    "    \n",
    "            wer = calculate_wer(normalize_text(transcriptions[audio_file]), normalize_text(recognized_text))\n",
    "            wers.append(wer)\n",
    "\n",
    "            final_report.append({\n",
    "                'Model': model_name,\n",
    "                'Language': language,\n",
    "                'File': audio_file,\n",
    "                'Time Spent, ms': int(np.round(time_spent_ms)),\n",
    "                'wer': f'{int(np.round(wer * 100))}%'\n",
    "            })\n",
    "\n",
    "        final_report.append({\n",
    "            'Model': model_name,\n",
    "            'Language': language,\n",
    "            'File': '*',\n",
    "            'Time Spent, ms': int(np.round(np.mean(time_spents))),\n",
    "            'wer': f'{int(np.round(np.mean(wers) * 100))}%'\n",
    "        })\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.DataFrame(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "de36ad92259e7fda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
